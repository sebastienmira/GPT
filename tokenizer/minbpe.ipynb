{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! a12a12a12a12a1212ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(tokens, counts = None): #organizes tokens into a lidictionary with respective num of appearances in the text\n",
    "    counts={}  if counts is None else counts\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx): # in a sequence of tokens substitute a pair of tokens by a new individual token idx\n",
    "    newids=[]\n",
    "    i=0\n",
    "    while i<len(ids):\n",
    "        if ids[i] == pair[0] and i < len(ids)-1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        \n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i+=1\n",
    "    return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer:\n",
    "    def __init__(self):      \n",
    "        self.merges = {}\n",
    "        self.vocab = {idx:bytes([idx]) for idx in range(256)}\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        tokens = list(text.encode(\"utf-8\"))        \n",
    "        \n",
    "        num_merges = vocab_size - 256\n",
    "        for i in range(num_merges):\n",
    "            stats = get_stats(tokens)\n",
    "            top_pair = max(stats, key=stats.get) #returns most common pair\n",
    "            idx = 256 + i\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'Merging {top_pair} into {idx}')\n",
    "            \n",
    "            tokens = merge(tokens, top_pair, idx)\n",
    "            \n",
    "            self.merges[top_pair] = idx\n",
    "            self.vocab[idx] = self.vocab[top_pair[0]] + self.vocab[top_pair[1]]\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "        while len(tokens)>=2:\n",
    "            stats = get_stats(tokens)\n",
    "            pair = min(stats, key=lambda p:self.merges.get(p, float(\"inf\"))) #get the pair that has was merged first\n",
    "            if pair not in self.merges:\n",
    "                break #nothing else to merge\n",
    "            \n",
    "            idx = self.merges[pair]\n",
    "            tokens = merge(tokens, pair, idx)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = b\"\".join([self.vocab[idx] for idx in ids])\n",
    "        text = tokens.decode(\"utf-8\", errors='replace') #translates bytes to characters\n",
    "        return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging (101, 32) into 256\n",
      "Merging (240, 159) into 257\n",
      "Merging (226, 128) into 258\n",
      "Merging (105, 110) into 259\n",
      "Merging (115, 32) into 260\n",
      "Merging (97, 110) into 261\n",
      "Merging (116, 104) into 262\n",
      "Merging (257, 133) into 263\n",
      "Merging (257, 135) into 264\n",
      "Merging (97, 114) into 265\n",
      "Merging (239, 189) into 266\n",
      "Merging (258, 140) into 267\n",
      "Merging (267, 264) into 268\n",
      "Merging (101, 114) into 269\n",
      "[99, 265, 256]\n",
      "care \n"
     ]
    }
   ],
   "source": [
    "tok = BasicTokenizer()\n",
    "tok.train(text, 270, verbose=True)\n",
    "print(tok.encode('care '))\n",
    "print(tok.decode(tok.encode('care ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing GPT-4-like tokenizer\n",
    "import regex as re\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexTokenizer(BasicTokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.pattern = GPT4_SPLIT_PATTERN\n",
    "        self.compiled_pattern = re.compile(self.pattern)\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        \n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)        \n",
    "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "\n",
    "            stats = {}\n",
    "\n",
    "            for chunk_ids in ids:\n",
    "                get_stats(chunk_ids, stats)\n",
    "\n",
    "            top_pair = max(stats, key=stats.get) #returns most common pair\n",
    "            idx = 256 + i\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'Merging {top_pair} into {idx}')\n",
    "            \n",
    "            ids = [merge(chunk_ids, top_pair, idx) for chunk_ids in ids]\n",
    "            \n",
    "            self.merges[top_pair] = idx\n",
    "            self.vocab[idx] = self.vocab[top_pair[0]] + self.vocab[top_pair[1]]\n",
    "\n",
    "    def _encode_chunk(self, chunk_bytes):\n",
    "        ids = list(chunk_bytes)\n",
    "        while len(ids)>=2:\n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats, key=lambda p:self.merges.get(p, float(\"inf\"))) #get the pair that has was merged first\n",
    "            if pair not in self.merges:\n",
    "                break #nothing else to merge\n",
    "            \n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids, pair, idx)\n",
    "        \n",
    "        return ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        # chunks encoded separately and then merged together\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "\n",
    "        ids = []\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            chunk_bytes = chunk.encode(\"utf-8\")\n",
    "            chunk_ids = self._encode_chunk(chunk_bytes)\n",
    "            ids.extend(chunk_ids)\n",
    "        \n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = b\"\".join([self.vocab[idx] for idx in ids])\n",
    "        text = tokens.decode(\"utf-8\", errors='replace') #translates bytes to characters\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging (240, 159) into 256\n",
      "Merging (226, 128) into 257\n",
      "Merging (105, 110) into 258\n",
      "Merging (32, 97) into 259\n",
      "Merging (32, 116) into 260\n",
      "Merging (260, 104) into 261\n",
      "Merging (256, 133) into 262\n",
      "Merging (256, 135) into 263\n",
      "Merging (97, 114) into 264\n",
      "Merging (239, 189) into 265\n",
      "Merging (257, 140) into 266\n",
      "Merging (266, 263) into 267\n",
      "Merging (101, 114) into 268\n",
      "Merging (111, 114) into 269\n",
      "[99, 264, 101, 259, 49, 50, 33]\n",
      "care \n"
     ]
    }
   ],
   "source": [
    "tok = RegexTokenizer()\n",
    "tok.train(text, 270, verbose=True)\n",
    "print(tok.encode('care a12!'))\n",
    "print(tok.decode(tok.encode('care ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
