{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+Ng5rSa+bwgGfFeSGrS/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebastienmira/GPT/blob/main/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S91ixj4_Qihe",
        "outputId": "caff8cbb-fe00-452b-d082-59fbfb4dc587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2849, val loss 4.2823\n",
            "step 300: train loss 2.3183, val loss 2.3466\n",
            "step 600: train loss 1.8885, val loss 2.0019\n",
            "step 900: train loss 1.6504, val loss 1.8089\n",
            "step 1200: train loss 1.5188, val loss 1.7069\n",
            "step 1500: train loss 1.4394, val loss 1.6403\n",
            "step 1800: train loss 1.3758, val loss 1.5862\n",
            "step 2100: train loss 1.3274, val loss 1.5594\n",
            "step 2400: train loss 1.2909, val loss 1.5387\n",
            "step 2700: train loss 1.2561, val loss 1.5145\n",
            "step 3000: train loss 1.2273, val loss 1.5077\n",
            "step 3300: train loss 1.2016, val loss 1.4953\n",
            "step 3600: train loss 1.1810, val loss 1.4953\n",
            "step 3900: train loss 1.1546, val loss 1.4923\n",
            "step 4200: train loss 1.1311, val loss 1.4858\n",
            "step 4500: train loss 1.1064, val loss 1.4814\n",
            "step 4800: train loss 1.0873, val loss 1.4943\n",
            "\n",
            "But with prison: I with such sweet more\n",
            "merely.\n",
            "\n",
            "MENENIUS:\n",
            "Sir, 'tis hi nothing. He wish'd to slep to\n",
            "can cold him win his inher.\n",
            "\n",
            "MISTRESS OVERDONE:\n",
            "The last go, these weary harms all woman.\n",
            "Go: I hear home to the world: whatsures she was?\n",
            "\n",
            "Both Trio mark, look upon the garlds, Juliet? Hence, who feeling,\n",
            "thereto with the helds of Lord:\n",
            "Then spice, it was not much seen that looks his\n",
            "speak; there, appearing. Here's sleptency, matues\n",
            "have standed, In England as you!\n",
            "\n",
            "Lord Soldier:\n",
            "Look, together\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 300\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384 #number of embedding dimensions\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "#characters that appear in the text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "#mapping of the chars to ints and vice-versa\n",
        "stoi = { ch:i for i,ch in enumerate(chars)}\n",
        "itos = { i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "#splitting the data\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "#data loading\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
        "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x,y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X,Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    #one head of self-attention\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        #compute affinities\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei) # prevents some nodes to communicates to prevent\n",
        "        #weighted aggregation\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    #multiple attention heads in parallel\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out)) # projection back into residual pathway\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    #Linear layer + Non-linearity\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), #multiply by 4 as in the paper\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), #projection layer into residual pathway\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    #Transformer block: communication(attention) followed by computation (ffwd)\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) #normalizing and implementing residual connections\n",
        "        x = x + self.ffwd(self.ln2(x)) #normalizing and implementing residual connections\n",
        "        #in the paper normalization comes after the layer but we implemented this reversed as it became more common\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #each token reads logits for next tkn from lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        #self.sa_head = Head(n_embd)\n",
        "        #self.sa_heads = MultiHeadAttention(4, n_embd//4) # 4 heads with 8-dim self-attention. 4 communication channels in parallel\n",
        "        #self.ffwd = FeedForward(n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are (B, T) tensor of ints (Batch, Time, Channel)\n",
        "        #in this context time represents the sequential nature of the data (block_size). Channel is representative of the logits for each token in the embedding table(vocab_size)\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C) C=n_embd\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # (T,C)\n",
        "        x = tok_emb + pos_emb #(B,T,C) x holds the token and positional identity\n",
        "        #x = self.sa_head(x) #apply one head of self-attention (B,T,C)\n",
        "        #x = self.sa_heads(x) #apply multi-head of self-attention (B,T,C)\n",
        "        #x = self.ffwd(x) #(B,T,C)\n",
        "        x = self.blocks(x) #(B,T,C)\n",
        "        x = self.ln_f(x) #final layer of normalization (B,T,C)\n",
        "        logits = self.lm_head(x) #(B,T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "        #we reshape the logits and targets in order to use the cross_entropy function\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        #idx is (B,T) array of indices in current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            #crop idx to block_size\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            #get predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            #focus on last time step (last element)\n",
        "            logits = logits[:,-1,:] #becomes (B,C)\n",
        "            #apply softmax\n",
        "            probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "            #sample from prob distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "            #append sampled idx to the sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#training\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    #sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
      ],
      "metadata": {
        "id": "29q5h5B9QuuH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53e42fb-29d3-4a3b-e5f0-a54ef2538375"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ISABELLA:\n",
            "I do long them.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "When I comes you here? did me but rie to church now.\n",
            "\n",
            "ISABELLA:\n",
            "Dear smake that lives.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Lost by, my lord, I'll prompt you with you.\n",
            "You, sir, my hands and tell me on will sew me follow.\n",
            "But meet you my son, you are power more prompt.\n",
            "Now, very wedding the consuls\n",
            "When after mews than nature; hath you but brought\n",
            "The general portuned in stay.\n",
            "\n",
            "GLOUCESTER:\n",
            "The king of your lordship's joy, ere I die to the form,\n",
            "Upon which you have been! Betwarray this arm,\n",
            "It is full offitance, removed at law,\n",
            "Be the king sickly of Warwicks, Edward is my son,\n",
            "And further, like Some tail despister'd ation,\n",
            "And Lord Such lewishing the things of blessed Phoeble,\n",
            "The lebost, he cannot from himself\n",
            "Fively chancelling her ever virgiously;\n",
            "But not do confess corrates him proceeding.\n",
            "Well them choose us I do not. Look,\n",
            "For I good to light not such fortune bench:\n",
            "Then by actions must I see i' the diving of such\n",
            "good friends a visage, or aggility. Is leve,\n",
            "Supply meltitude. I is mellow,--devount? If mind\n",
            "My soun soverely dark, I know not swear\n",
            "The dagger, no warrant for me with one honour\n",
            "As forgiveness\n",
            "Witness may yet or else thou hast me intent\n",
            "Would rathen yet but posely\n",
            "The rests,\n",
            "The plants of roservel and present\n",
            "Ayes not well, yet the husband's socks: as birtten'd\n",
            "'Sixture\n",
            "We shall aunstrengthen fetch a watch; how for a vaint\n",
            "Made things shriving lack in their gelts,\n",
            "With the empty passion lute themselves, why,\n",
            "Wave thy blusters lumberity lords, and now\n",
            "rehends our swear,\n",
            "In receive the sudden minds offenderst puts\n",
            "Will content my solting or brows;\n",
            "Thy noblece impression that he attorned.\n",
            "To whip, have these attends again and home,\n",
            "Comes we followed put to added things entrary.\n",
            "Go, thy brother, as do direct me safe;\n",
            "'Tis weaking me fram which in one eyes,\n",
            "That vasting Henry taks these fresh in lengthen's Biona,\n",
            "That extrans with the dull studges of 'moisely.\n",
            "\n",
            "KING RICHARD II:\n",
            "Say, all company this is alloge, and slanger,\n",
            "There the Seek taker? O pate-sime hearty;\n",
            "And stilly privy here and here in the woest-stang\n",
            "'Rome of Richard, and still Romeo,\n",
            "To behold thy wandering covertuRe.\n",
            "\n",
            "Third Servingman:\n",
            "No, four thence; yet that did excuse\n",
            "The season of our tongue grace thee and who haste,\n",
            "To stouds your honesty are him! That you might in praction.\n",
            "\n",
            "MARIANA:\n",
            "Now, Joint! for madam.\n",
            "\n",
            "ANGELO:\n",
            "So is that she put this beast aborn before\n",
            "I salved thee.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Do you pleases?\n",
            "\n",
            "ESCALUS:\n",
            "I hope.\n",
            "\n",
            "ESBY:\n",
            "Troy, my lord; yet know, what no hear is said?\n",
            "\n",
            "LUCIO:\n",
            "As if it betrike.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Thinkest, wough it malice the day?\n",
            "\n",
            "ISABELLA:\n",
            "Therefore is thee.\n",
            "Thou hast news doubt: art thou into their duty!\n",
            "\n",
            "LUCIO:\n",
            "Why, farest, if you went thyself, thy uncle spoits night.\n",
            "\n",
            "OLUCIO:\n",
            "Pray, thither and I thought not, am thus mock now.\n",
            "\n",
            "ISABELLA:\n",
            "I have brought thee, my hasteress thee and daughter,\n",
            "Yet thou art not fit better o' the part.\n",
            "\n",
            "LUCIO:\n",
            "By day, sir: you lay before so, let's be banished\n",
            "Thy bralic fodds, petting the fairity purpose:\n",
            "Yet norgot in the with thine issues the vent.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "By sovereignce we carry to do you so?\n",
            "I will, then levis of such such a general\n",
            "As thousand poor ben, by some other lains-let see\n",
            "scurriture's desposed; which whereby,\n",
            "He may hadst abused his piblicles, he surpect,\n",
            "Cald alour to be deal hence, we were assess\n",
            "craved at the that of treaty Richard: ears these\n",
            "hough rises let atorne be our perse.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "How! there didst you swears\n",
            "Of all perform lengthen me, but I must\n",
            "Indeslike my wipe in that fancy; withing of you,\n",
            "'Tis methinks, it pleased, pie to myself\n",
            "To see you unto endure the Margaret:\n",
            "Then let me set my comfort live, thoughtst have with hang\n",
            "As fronted with Paris' flood.'\n",
            "Now those fellows:\n",
            "If thou speak; tradiciousls ere furthen love.\n",
            "Thy ueers and Some is she should rust:\n",
            "Forgery, that are crafted begin the Glisterey\n",
            "And lived the breach on my heart-hours knight.\n",
            "\n",
            "GRUMIO:\n",
            "Consperit, cares are thy hand:\n",
            "The circules the more than be his liege.\n",
            "Within till thou lie me?\n",
            "\n",
            "MISTRESS OVERDOLAND:\n",
            "Thy stewaby Duke of Norfolk,\n",
            "Whilest thou shot goest me have been me.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "O Clarence, sir, what uncle it slays,\n",
            "So so party withhout I true this curse could;\n",
            "They haught was'd me to use offender;\n",
            "Conato not unpropted Velscians,\n",
            "When but crown'd from yond elvish'd thereft thy hands;\n",
            "O, what to be Escuivioli th,\n",
            "By rave my spirits, snow to-day, by trefights,\n",
            "Not with me, swear gifts ne'er say thy firs;\n",
            "For thy blood children for farm, and men Mistress'd speak,\n",
            "Who'st thy most ruely child?\n",
            "\n",
            "Nurse:\n",
            "Faith, gentle thou beguis thy free, corse!\n",
            "\n",
            "CLARENCE:\n",
            "Well, well! I know not what he betids:\n",
            "He will not call thee--I have Edward, many scepture,\n",
            "Nor knows nor she was children'd corres to Richard:\n",
            "Thou, woo clargy, his love to like the same;\n",
            "We hath goes in roan to that served y thyself;\n",
            "Thy first court\n",
            "Washic termed to put mine. Art leagues!\n",
            "I will now, holdier, now thy clubs are thy face,\n",
            "As a pledgerous might to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qSHAXZMEnpKn"
      },
      "execution_count": null,
      "outputs": []
    }
<<<<<<< HEAD
   ],
   "source": [
    "#Version 2\n",
    "#we use a matrix multiplication trich to average the values. Multiply x by a triangular matrix where rows are normalized.\n",
    "# we basically do weighted sums of the tokens preceeding\n",
    "wei = torch.tril(torch.ones(T,T)) #triangular matrix of ones\n",
    "wei = wei/torch.sum(wei, 1, keepdim= True)\n",
    "xbow2 = wei @ x # (B,T,T) @ (B,T,C) --> (B,T,C)\n",
    "xbow2[0]\n",
    "#same outcome but more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3\n",
    "#Using Softmax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # where tril == 0 wei becomes -inf\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "xbow3[0]\n",
    "#same result but allows for weights to be updated\n",
    "#this weights measures how much tokens from the past influence (some sort of affinity) and can be trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 4: self-attention\n",
    "torch.manual_seed(1337)\n",
    "B, T, C= 4, 8, 32\n",
    "x=torch.randn(B,T,C)\n",
    "\n",
    "#Single Head performing Self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B,T,head_size)\n",
    "q = query(x) # (B,T,head_size=16)\n",
    "wei = q @ k.transpose(-2, -1) # (B,T,16) @ (B,16,T) --> (B,T,T)\n",
    "#wei =  q @ k.transpose(-2, -1) * head_size**-0.5    scaled self-attention\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # where tril == 0 wei becomes -inf\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape\n",
    "\n",
    "#token emits a query to each previous tokens \"reply\" with keys.\n",
    "# keys and queries interact to define affinities between tokens (wei). \n",
    "#attention is a communication mecanism between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
=======
  ]
}
>>>>>>> 9bc45d565629c6ba9afcdf64de3d56e1392f7efc
